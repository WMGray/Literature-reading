# ProtTrans: Towards Cracking the Language of Lifes Code Through Self-Supervised Learning

# Introduction
  - 计算生物学和生物信息学中的大量的蛋白质序列数据可供NLP提取语言模型。
  - 本文训练了两个自回归模型：Transformer-XL和XLNet；四个自动编码器模型：BERT , Albert, Electra, T5。这些模型的数据来自于UniRef和BFD，包含多大3930亿个氨基酸。
  - 训练配置：Summit超级计算机，使用5616个GPU和至多1024个TPU Pod核心。
  - **降维**表明，未标记数据中的原始蛋白质语言模型嵌入捕捉到了蛋白质序列的一些生物物理特征。
  - 嵌入的优势：
    1. 蛋白质二级结构的逐残基预测：三态精度Q3=81%-87%
    2. 蛋白质亚细胞定位的逐蛋白预测:十态精度Q10=81%
    3. membrane vs. water-soluble：两态精度Q2=91%
  - 在不适用进化信息的情况下，使用嵌入的ProtT5首次超过了最先进的技术，避免了耗费大的数据库检索。
  - 自回归模型
    ![自回归模型](./images/PFP/ProtTrans/%E8%87%AA%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B.png)
  - 自动编码器模型：[参考](https://zhuanlan.zhihu.com/p/84533223)
# Introduction
  - 更强大的超级计算机和高级库可以使用GPU和TPU在更大的数据集上训练更复杂的模型。
  - 在训练之后，提取一些从语言模型中学到的信息称为**嵌入**。
  - 蛋白质由20种不同的氨基酸构成。[蛋白质结构](cnblogs.com/jiangxinyang/p/10069330.html)
  - 通过基于人工智能(AI)的预测方法来弥合序列注释差 距是计算生物学和生物信息学的关键挑战之一。
  - 一些顶级预测方法结合了机器学习和进化信息（EI）。
  - EI在生物序列数据库是充分可用的，然而也有一些问题：  
    1. 当预测整个蛋白质组时，编译所有的进化信息计算代价巨大；
    2. EI并非对所有的蛋白质都有效（本体紊乱蛋白、暗蛋白组）；
    3. EI类型多样化时，提升是最好的；
    4. 基于EI的预测方法在无法区分同一个家族的两个蛋白质之间的差异。
  - 目前AlphaFold2在预测蛋白质三维结构的准确性达到了前所未有的水平，相应的，它的计算成本也是巨大的。

# Method

## 蛋白质语言模型数据
  - 数据集：Uniref50、Uniref100和BFD
  - BFD合并了UniProt和从多个宏基因组测序项目翻译的蛋白质
  - 总的来说，BFD比以前蛋白质语言模型用的最大数据集大约大8倍，token数量增加5倍(Uniref100序列比BFD长1.6倍)。[表1]
  - 将单个氨基酸作为输入单词/token，因此蛋白质数据库所包含的token比NLP中使用的与语料库多几个数量级。
  - Uniref50、UniRef100和BFD在每个标记之间使用单个空格（指示词边界）进行标记。
  - 每个蛋白质序列存在单独的行中，相当于句子。
  - 在每个蛋白质序列之间插入以个空行，以表示"文档结尾"
    ![数据集](./images/PFP/ProtTrans/%E6%95%B0%E6%8D%AE%E9%9B%86.png)

## 用于监督训练的嵌入
  - 通过嵌入的方式提取蛋白质语言模型的信息，即蛋白质模型最后一个隐藏状态的向量表示。在迁移学习中，这些嵌入将作为后续监督训练的输入。
  - 逐残基预测/单个标记：预测单个token的性质（这里是单个氨基酸，加入蛋白质时称为残基）
  - 逐蛋白预测/嵌入池(embedding pooling)：对整个蛋白质的特征进行预测，相当于NLP中的整句分类。用于将蛋白质分为膜结合与水溶性和十类亚细胞
  - 特征提取概述：如何使用ProtTrans模型在氨基酸或整个蛋白质水平上为任意蛋白质序列提取特征
    1. 首先，对任一个蛋白质序列"SEQ"进行标记(tokenization)并添加位置编码
    2. 生成的向量通过ProtTrans模型来为每个输入标记(氨基酸)创建上下文感知嵌入。我们使用Transformer注意堆栈的最后一个隐藏状态作为下游预测方法的输入。
    3. 这些嵌入(来自2)可以作为单个token级别的预测任务的输入。eg：CNN——预测氨基酸的二级结构
    4. 这些嵌入(来自2)也可以沿着长度维度进行连接和池化，来获得固定大小的蛋白质嵌入，生成的蛋白质嵌入可用于预测蛋白质当面的输入。eg：FNN——用于蛋白质的细胞定位
    ![特征提取概述](./images/PFP/ProtTrans/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E6%A6%82%E8%BF%B0.png)

## 用于嵌入的非监督评估的数据
  - 通过使用t-SNE将高维表示投影到二维来评估从蛋白质语言模型中提取的嵌入所捕获的信息。
    1. 从数据获取注释。数据集：SCOPe数据集中的非冗余版本(PIDE<40%)，包含14323种蛋白质
    2. 将蛋白质映射到生物的三个主要领域（古细菌、细菌和真核生物）或病毒（删除所欲缺少分类的蛋白质）
    3. t-SNE预测的迭代次数设置为3000，除氨基酸图（困惑都设置为5），其他图的困惑度设置为30，所有可视化使用相同的随机种子。

## 步骤1：蛋白质语言模型提取嵌入
  - 在蛋白质序列上训练了6个语言模型：T5、Electra、BERT、Albert、Transformer-XL、XLNet
  - BERT模型是NLP中第一个试图重建被破坏的token的双向模型，被认为是NLP迁移学习的实际标准。
  - Albert通过注意力层之间的硬参数共享降低了BERT的复杂性，这允许增加注意力头的数量（这里选择了64个）。
  - Electra通过训练两个网络，一个生成器和一个判别器来提高预训练任务的采样效率。生成器(BERT)不仅重建损坏的输入tokens，还重建屏蔽tokens，从而可能创建合理的替代方案，鉴别器(Electra)检测哪些tokens被屏蔽。
  - T5 使用为序列翻译提出的原始转换器架构，该架构由一个编码器(将源语言投影到嵌入空间)和一个解码器(基于编码器嵌入生成目标语言的翻译)组成。后来，模型使用了编码器(BERT, Albert, Electra)或解码器(TransformerXL, XLNet)。提供了应用不同的训练方法和不同的掩码策略的灵活性，例如，T5允许重建tokens的跨度而不是单个tokens。
  - self-attention是一种集合操作，与顺序无关，因此Transformer需要显式的位置编码。使用正弦位置信号训练的模型（如 BERT、Albert 或 Electra）只能处理更短或等于在训练前设置的位置编码长度的序列。
  - 由于Transformer模型对内存需求较大，所以通常将长度参数设置为比最长的蛋白质低的值。
  - 首先在长度≤512的蛋白质上训练一些模型(Bert、AlBert、Electra)，然后在长度≤1024的蛋白质上训练。
  - 在Uniref100上寻来你ProtXL、ProtBert、ProtXLNet、ProtAlbert和ProtElectra，在Uniref上训练ProtT5，在BFD上训练ProtXL、ProtBert和ProtT5
  - ProtXL：Transformer-XL使用Uniref100和BFD100数据集，分别称为ProtXL和ProtXL-BFD。两种模型都使用15%的dropout率，内存长度为512个tokens，并使用混合精度。其他参数根据训练集大小和GPU利用率进行调整。（1）对于BFD学习率随着warm-up步骤减少，或（2）对于Uniref100两者都增加。
  - ==ProtBert==
  - ProtAlbert：在数据集Uniref100上训练Albert，使用官方Github Albert库(版本：xxlarge:2)的配置。**对于Albert来说，层的数量随着次数的增加而增加，Albert堆叠其单层**。相较于原始版本，我们在相同的硬件配置上训练模型的全局batch size 从4096增加到了10753。       
    ProtAlbert首先在最大长度为512的序列上进行150k步计算，然后在最大长度为2k的长度上进行150k步计算。

  - ==ProtXLNet==
  - ==ProtElectra==
  - ==ProtT5==
  - 模型参数设置
    ![模型参数设置](./images/PFP/ProtTrans/%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE.png)

## 步骤2：监督模型的迁移学习
  - 为了更好的分析迁移学习的影响，我们选择使用来自蛋白质语言模型的嵌入作为输入来确保监督模型的最小化。
  - 与NetSufP-2.0等SOA方法相比，我们实验都是使用预先训练的语言模型作为特征提取器，并没有进行微调，即没有梯度反向传播到语言模型。
  - 从预训练语言模型的最后一个隐藏状态中提取了嵌入，并将任务应用到两个不同级别的层次上：
    1. 单个token(每残基)的预测：将嵌入输入到一个两层的CNN网络中，第一个CNN使用大小为7的窗口将嵌入压缩到32维。压缩后的表示被输入到两个不同的CNN（窗口大小为7）中，分别学会预测3态的二级结构和8态的二级结构，通过将两种输出的损失相加（多任务学习）来同时训练网络。
    2. 池化（每蛋白质）预测整个句子
  - 还训练了其他三个模型：逻辑回归、FNN和LSTM。
  - 与CNN类似，双层FNN首先将LMs的输出压缩到32维，第二层FNN用于同时预测三态和八态。
  - 双向LSTM将嵌入压缩到16维，将两个方向连接起来，得到的32维表示被FNN层用于预测三态或八态。
  - 对于每蛋白质的预测，从蛋白质语言模型的最后一个隐藏状态提取了嵌入，然后将长度-维度上的表示汇集在一起，得到所有蛋白质的固定大小的表示。比较了不同的池化策略，选择均值池化进行所有下一步的实验。经过池化产生的向量被用作单个前馈层的输入，该前馈层有32个神经元，对信息进行压缩，然后同时对多个蛋白质任务进行预测（亚细胞定位以及膜结合蛋白质和水溶性蛋白质之间的区别）。

## 硬件
  - 使用Summit对深度学习模型进行训练，使用Rhea对包括分布式生成的tensorflow records在内的数据集进行预处理。
  - 谷歌 TPU Pod
## 软件
  - Pytorch：ProtXL
  - tensorflow：ProtBert、ProtAlbert、ProtXLNet、ProtElectra、ProtT5
    ![通信开销](./images/PFP/ProtTrans/%E9%80%9A%E4%BF%A1%E5%BC%80%E9%94%80.png)

# Results

## 步骤1：无监督的蛋白质语言模型信息丰富
  - **捕获氨基酸的生物物理特征**，所有LM都捕获了基本的生物物理氨基酸特征，包括电荷，极性、大小、疏水性，甚至脂肪族和芳香族的水平。
  - 将嵌入投影与相同架构的随机初始化模型进行比较，随机投影显然不携带生物物理信息，而嵌入投影携带(生物遗传信息)。
  - **捕获蛋白质类**，为了评估无监督语言模型捕获了蛋白质的哪些方面，我们对每个模型的最后一层生成的表示的长度维度进行平均（参见图1）。通过SCOPe数据库注释了结构类。
  - **捕获生命和病毒域**，该分析区分了生命的三个领域：古细菌、细菌和真核生物，以及通常不被认为是生命的病毒。我们使用与 SCOPe分析相同的蛋白质和每个蛋白质池。所有蛋白质 LM 都捕获了一些生物体特异性方面。真核生物和细菌比病毒和古细菌聚集得更好。
  - **在保守结构中捕获蛋白质功能**。我们主要关注了每个transformer模型的核心注意力机制。==3.1 ProtAlbert==

## 步骤2 嵌入好的输入来进行预测
  - 从蛋白质语言模型中提取重要约束条件的嵌入的关键方法是专门使用嵌入层作为输入，对反映结构和功能的特征进行监督训练。
  - 通过两个不同的水平进行预测，蛋白质语言模型仅作为静态特征提取器。

### 逐残基二级结构预测
### 逐蛋白定位和膜预测

# 下一步工作
  1. 补充其他模型  
  2. ProtAlbert 的关注权重 SOM 图 11
